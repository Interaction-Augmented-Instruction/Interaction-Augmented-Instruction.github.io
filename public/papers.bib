@inproceedings{Chen2024d,
address = {New York, NY, USA},
author = {Chen, Liuqing and Xiao, Shuhong and Chen, Yunnong and Song, Yaxuan and Wu, Ruoyu and Sun, Lingyun},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642229},
file = {:C$\backslash$:/mendeley/Chen et al. - 2024 - ChatScratch An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--19},
publisher = {ACM},
title = {{ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642229},
year = {2024}
}
@inproceedings{Kim2023c,
address = {New York, NY, USA},
author = {Kim, Jeongyeon and Suh, Sangho and Chilton, Lydia B and Xia, Haijun},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
doi = {10.1145/3563657.3595996},
file = {:C$\backslash$:/mendeley/Kim et al. - 2023 - Metaphorian Leveraging Large Language Models to Support Extended Metaphor Creation for Science Writing.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces,VIS/GenAI Interfaces/scoped instruction/tools},
month = {jul},
pages = {115--135},
publisher = {ACM},
title = {{Metaphorian: Leveraging Large Language Models to Support Extended Metaphor Creation for Science Writing}},
url = {https://dl.acm.org/doi/10.1145/3563657.3595996},
year = {2023}
}
@article{Wang2023a,
author = {Wang, Chenglong and Thompson, John and Lee, Bongshin},
doi = {10.1109/TVCG.2023.3326585},
file = {:C$\backslash$:/mendeley/Wang, Thompson, Lee - 2024 - Data Formulator AI-powered Concept-driven Visualization Authoring.pdf:pdf},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {AI,Data visualization,Histograms,Libraries,Temperature distribution,Transforms,Urban areas,Visualization,data transformation,large language model,natural language,programming by example,visualization authoring},
mendeley-groups = {VIS/Author tool/Example,VIS/Author tool,VIS/GenAI Interfaces/scoped instruction/tools},
number = {1},
pages = {1128 -- 1138},
title = {{Data Formulator: AI-powered Concept-driven Visualization Authoring}},
url = {https://ieeexplore.ieee.org/document/10292609/},
volume = {30},
year = {2024}
}
@inproceedings{Suh2023,
address = {New York, NY, USA},
author = {Suh, Sangho and Chen, Meng and Min, Bryan and Li, Toby Jia-Jun and Xia, Haijun},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642400},
file = {:C$\backslash$:/mendeley/Suh et al. - 2024 - Luminate Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools,VIS/GenAI Interfaces/HAI},
month = {may},
pages = {1--26},
publisher = {ACM},
title = {{Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642400},
year = {2024}
}
@inproceedings{Singh2024,
address = {New York, NY, USA},
author = {Singh, Nikhil and Wang, Lucy Lu and Bragg, Jonathan},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
doi = {10.1145/3640543.3645212},
file = {:C$\backslash$:/mendeley/Singh, Wang, Bragg - 2024 - FigurA11y AI Assistance for Writing Scientific Alt Text.pdf:pdf},
isbn = {9798400705083},
keywords = {Accessibility,Alt text,Human-AI interaction,Image descriptions,Large language models,Natural language generation,Scientific figures,Writing assistance systems},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces,VIS/GenAI Interfaces/scoped instruction/tools},
month = {mar},
pages = {886--906},
publisher = {ACM},
title = {{FigurA11y: AI Assistance for Writing Scientific Alt Text}},
url = {https://dl.acm.org/doi/10.1145/3640543.3645212},
year = {2024}
}
@inproceedings{Wang2024,
address = {New York, NY, USA},
author = {Wang, Bryan and Li, Yuliang and Lv, Zhaoyang and Xia, Haijun and Xu, Yan and Sodhi, Raj},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
doi = {10.1145/3640543.3645143},
file = {:C$\backslash$:/mendeley/Wang et al. - 2024 - LAVE LLM-Powered Agent Assistance and Language Augmentation for Video Editing.pdf:pdf},
isbn = {9798400705083},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools,VIS/Animation{\&}Video/editing,VIS/Animation{\&}Video/CV},
month = {mar},
pages = {699--714},
publisher = {ACM},
title = {{LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing}},
url = {https://dl.acm.org/doi/10.1145/3640543.3645143},
year = {2024}
}

@article{Zhang2024,
archivePrefix = {arXiv},
arxivId = {2410.10570},
author = {Zhang, Rui and Zhang, Ziyao and Zhu, Fengliang and Zhou, Jiajie and Rao, Anyi},
eprint = {2410.10570},
file = {:C$\backslash$:/mendeley/Zhang et al. - 2024 - Mindalogue LLM-Powered Nonlinear Interaction for Effective Learning and Task Exploration.pdf:pdf},
journal = {arXiv},
keywords = {Large Language Models (LLM), Non-linear Interactio},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--17},
publisher = {ACM},
title = {{Mindalogue: LLM-Powered Nonlinear Interaction for Effective Learning and Task Exploration}},
url = {http://arxiv.org/abs/2410.10570},
year = {2024}
}
@inproceedings{Fok2024,
abstract = {Navigating the vast scientific literature often starts with browsing a paper's abstract. However, when a reader seeks additional information, not present in the abstract, they face a costly cognitive chasm during their dive into the full text. To bridge this gap, we introduce recursively expandable abstracts, a novel interaction paradigm that dynamically expands abstracts by progressively incorporating additional information from the papers' full text. This lightweight interaction allows scholars to specify their information needs by quickly brushing over the abstract or selecting AI-suggested expandable entities. Relevant information is synthesized using a retrieval-augmented generation approach, presented as a fluid, threaded expansion of the abstract, and made efficiently verifiable via attribution to relevant source-passages in the paper. Through a series of user studies, we demonstrate the utility of recursively expandable abstracts and identify future opportunities to support low-effort and just-in-time exploration of long-form information contexts through LLM-powered interactions.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2310.07581},
author = {Fok, Raymond and Chang, Joseph Chee and August, Tal and Zhang, Amy X. and Weld, Daniel S.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3654777.3676397},
eprint = {2310.07581},
file = {:C$\backslash$:/mendeley/Fok et al. - 2023 - Qlarify Recursively Expandable Abstracts for Directed Information Retrieval over Scientific Papers.pdf:pdf},
isbn = {9798400706288},
mendeley-groups = {VIS/EDU/SRL,VIS/EDU/paper reading,VIS/GenAI Interfaces/scoped instruction/tools,VIS/EDU,VIS/EDU/AI4Research},
month = {oct},
pages = {1--21},
publisher = {ACM},
title = {{Qlarify: Recursively Expandable Abstracts for Directed Information Retrieval over Scientific Papers}},
url = {http://arxiv.org/abs/2310.07581},
year = {2023}
}
@inproceedings{Bursztyn2021,
address = {New York, NY, USA},
author = {Bursztyn, Victor S. and Healey, Jennifer and Koh, Eunyee and Lipka, Nedim and Birnbaum, Larry},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3411763.3451596},
file = {:C$\backslash$:/mendeley/Bursztyn et al. - 2021 - Developing a Conversational Recommendation Systemfor Navigating Limited Options.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--6},
publisher = {ACM},
title = {{Developing a Conversational Recommendation Systemfor Navigating Limited Options}},
url = {https://dl.acm.org/doi/10.1145/3411763.3451596},
year = {2021}
}

@inproceedings{Petridis2024,
address = {New York, NY, USA},
author = {Petridis, Savvas and Wedin, Benjamin D. and Wexler, James and Pushkarna, Mahima and Donsbach, Aaron and Goyal, Nitesh and Cai, Carrie J. and Terry, Michael},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
doi = {10.1145/3640543.3645144},
file = {:C$\backslash$:/mendeley/Petridis et al. - 2024 - ConstitutionMaker Interactively Critiquing Large Language Models by Converting Feedback into Principles.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {mar},
pages = {853--868},
publisher = {ACM},
title = {{ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles}},
url = {https://dl.acm.org/doi/10.1145/3640543.3645144},
year = {2024}
}
@inproceedings{Feng2024,
address = {New York, NY, USA},
author = {Feng, Li and Yen, Ryan and You, Yuzhe and Fan, Mingming and Zhao, Jian and Lu, Zhicong},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642212},
file = {:C$\backslash$:/mendeley/Feng et al. - 2024 - CoPrompt Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--21},
publisher = {ACM},
title = {{CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642212},
year = {2024}
}
@inproceedings{Suh2023a,
address = {New York, NY, USA},
author = {Suh, Sangho and Min, Bryan and Palani, Srishti and Xia, Haijun},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606756},
file = {:C$\backslash$:/mendeley/Suh et al. - 2023 - Sensecape Enabling Multilevel Exploration and Sensemaking with Large Language Models.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--18},
publisher = {ACM},
title = {{Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606756},
year = {2023}
}
@inproceedings{Cai2024,
address = {Stroudsburg, PA, USA},
author = {Cai, Yuzhe and Mao, Shaoguang and Wu, Wenshan and Wang, Zehua and Liang, Yaobo and Ge, Tao and Wu, Chenfei and WangYou, WangYou and Song, Ting and Xia, Yan and Duan, Nan and Wei, Furu},
booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)},
doi = {10.18653/v1/2024.naacl-demo.2},
file = {:C$\backslash$:/mendeley/Cai et al. - 2024 - Low-code LLM Graphical User Interface over Large Language Models.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
pages = {12--25},
publisher = {ACL},
title = {{Low-code LLM: Graphical User Interface over Large Language Models}},
url = {https://aclanthology.org/2024.naacl-demo.2},
year = {2024}
}
@inproceedings{Gero2022,
address = {New York, NY, USA},
author = {Gero, Katy Ilonka and Liu, Vivian and Chilton, Lydia},
booktitle = {Designing Interactive Systems Conference},
doi = {10.1145/3532106.3533533},
file = {:C$\backslash$:/mendeley/Gero, Liu, Chilton - 2022 - Sparks Inspiration for Science Writing using Language Models.pdf:pdf},
isbn = {9781450393584},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces,VIS/GenAI Interfaces/scoped instruction/tools},
month = {jun},
pages = {1002--1019},
publisher = {ACM},
title = {{Sparks: Inspiration for Science Writing using Language Models}},
url = {https://dl.acm.org/doi/10.1145/3532106.3533533},
year = {2022}
}
@inproceedings{Jiang2023,
address = {New York, NY, USA},
author = {Jiang, Peiling and Rayan, Jude and Dow, Steven P. and Xia, Haijun},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606737},
file = {:C$\backslash$:/mendeley/Jiang et al. - 2023 - Graphologue Exploring Large Language Model Responses with Interactive Diagrams.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools,VIS/GenAI Interfaces/HAI},
month = {oct},
pages = {1--20},
publisher = {ACM},
title = {{Graphologue: Exploring Large Language Model Responses with Interactive Diagrams}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606737},
year = {2023}
}
@inproceedings{Tilekbay,
address = {New York, NY, USA},
author = {Tilekbay, Bekzat and Yang, Saelyne and Lewkowicz, Michal Adam and Suryapranata, Alex and Kim, Juho},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces, IUI'24},
doi = {10.1145/3640543.3645164},
file = {:C$\backslash$:/mendeley/Tilekbay et al. - 2024 - ExpressEdit Video Editing with Natural Language and Sketching.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools,VIS/Animation{\&}Video/editing},
month = {mar},
pages = {515--536},
publisher = {ACM},
title = {{ExpressEdit: Video Editing with Natural Language and Sketching}},
url = {https://dl.acm.org/doi/10.1145/3640543.3645164},
year = {2024}
}
@inproceedings{Wu2022e,
address = {New York, NY, USA},
author = {Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3491101.3519729},
file = {:C$\backslash$:/mendeley/Wu et al. - 2022 - PromptChainer Chaining Large Language Model Prompts through Visual Programming.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {apr},
pages = {1--10},
publisher = {ACM},
title = {{PromptChainer: Chaining Large Language Model Prompts through Visual Programming}},
url = {https://dl.acm.org/doi/10.1145/3491101.3519729},
year = {2022}
}
@inproceedings{Aksan2018,
address = {New York, NY, USA},
author = {Aksan, Emre and Pece, Fabrizio and Hilliges, Otmar},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3173574.3173779},
file = {:C$\backslash$:/mendeley/Aksan, Pece, Hilliges - 2018 - DeepWriting Making digital ink editable via deep generative modeling.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces,VIS/GenAI Interfaces/scoped instruction/tools},
month = {apr},
pages = {1--14},
publisher = {ACM},
title = {{DeepWriting: Making digital ink editable via deep generative modeling}},
url = {https://dl.acm.org/doi/10.1145/3173574.3173779},
year = {2018}
}
@inproceedings{Tang2024,
address = {New York, NY, USA},
author = {Tang, Yi and Chang, Chia-Ming and Yang, Xi},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
doi = {10.1145/3640543.3645174},
file = {:C$\backslash$:/mendeley/Tang, Chang, Yang - 2024 - PDFChatAnnotator A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs.pdf:pdf},
isbn = {9798400705083},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {mar},
pages = {419--430},
publisher = {ACM},
title = {{PDFChatAnnotator: A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs}},
url = {https://dl.acm.org/doi/10.1145/3640543.3645174},
year = {2024}
}
@article{Liu2023c,
archivePrefix = {arXiv},
arxivId = {2305.05662},
author = {Liu, Zhaoyang and He, Yinan and Wang, Wenhai and Wang, Weiyun and Wang, Yi and Chen, Shoufa and Zhang, Qinglong and Lai, Zeqiang and Yang, Yang and Li, Qingyun and Yu, Jiashuo and Li, Kunchang and Chen, Zhe and Yang, Xue and Zhu, Xizhou and Wang, Yali and Wang, Limin and Luo, Ping and Dai, Jifeng and Qiao, Yu},
eprint = {2305.05662},
file = {:C$\backslash$:/mendeley/Liu et al. - 2023 - InternGPT Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language.pdf:pdf},
journal = {arXiv},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces,VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--17},
title = {{InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language}},
url = {http://arxiv.org/abs/2305.05662},
year = {2023}
}
@inproceedings{Vaithilingam2024,
address = {New York, NY, USA},
author = {Vaithilingam, Priyan and Glassman, Elena L. and Inala, Jeevana Priya and Wang, Chenglong},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642639},
file = {:C$\backslash$:/mendeley/Vaithilingam et al. - 2024 - DynaVis Dynamically Synthesized UI Widgets for Visualization Editing.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/NLI/NL for vis,VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--17},
publisher = {ACM},
title = {{DynaVis: Dynamically Synthesized UI Widgets for Visualization Editing}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642639},
year = {2024}
}
@inproceedings{Reza2024,
address = {New York, NY, USA},
author = {Reza, Mohi and Laundry, Nathan M and Musabirov, Ilya and Dushniku, Peter and Yu, Zhi Yuan “Michael” and Mittal, Kashish and Grossman, Tovi and Liut, Michael and Kuzminykh, Anastasia and Williams, Joseph Jay},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3641899},
file = {:C$\backslash$:/mendeley/Reza et al. - 2024 - ABScribe Rapid Exploration {\&}amp Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Larg.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--18},
publisher = {ACM},
title = {{ABScribe: Rapid Exploration & Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models}},
url = {https://dl.acm.org/doi/10.1145/3613904.3641899},
year = {2024}
}
@inproceedings{Chung2023,
address = {New York, NY, USA},
author = {Chung, John Joon Young and Adar, Eytan},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606777},
file = {:C$\backslash$:/mendeley/Chung, Adar - 2023 - PromptPaint Steering Text-to-Image Generation Through Paint Medium-like Interactions.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--17},
publisher = {ACM},
title = {{PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like Interactions}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606777},
year = {2023}
}
@inproceedings{Dogan2024,
address = {New York, NY, USA},
author = {Dogan, Mustafa Doga and Gonzalez, Eric J. and Ahuja, Karan and Du, Ruofei and Cola{\c{c}}o, Andrea and Lee, Johnny and Gonzalez-Franco, Mar and Kim, David},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3654777.3676379},
file = {:C$\backslash$:/mendeley/Dogan et al. - 2024 - Augmented Object Intelligence with XR-Objects.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--15},
publisher = {ACM},
title = {{Augmented Object Intelligence with XR-Objects}},
url = {https://dl.acm.org/doi/10.1145/3654777.3676379},
year = {2024}
}
@inproceedings{Almeda2024,
address = {New York, NY, USA},
author = {Almeda, Shm Garanganao and Zamfirescu-Pereira, J.D. and Kim, Kyu Won and {Mani Rathnam}, Pradeep and Hartmann, Bjoern},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642858},
file = {:C$\backslash$:/mendeley/Almeda et al. - 2024 - Prompting for Discovery Flexible Sense-Making for AI Art-Making with Dreamsheets.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--17},
publisher = {ACM},
title = {{Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642858},
year = {2024}
}
@inproceedings{Brade2023,
address = {New York, NY, USA},
author = {Brade, Stephen and Wang, Bryan and Sousa, Mauricio and Oore, Sageev and Grossman, Tovi},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606725},
file = {:C$\backslash$:/mendeley/Brade et al. - 2023 - Promptify Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--14},
publisher = {ACM},
title = {{Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606725},
year = {2023}
}
@inproceedings{Xie2024a,
address = {New York, NY, USA},
author = {Xie, Liwenhan and Zheng, Chengbo and Xia, Haijun and Qu, Huamin and Zhu-Tian, Chen},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, UIST'24},
doi = {10.1145/3654777.3676374},
file = {:C$\backslash$:/mendeley/Xie et al. - 2024 - WaitGPT Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--14},
publisher = {ACM},
title = {{WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization}},
url = {https://dl.acm.org/doi/10.1145/3654777.3676374},
year = {2024}
}
@article{Choe2024,
author = {Choe, Kiroong and Lee, Chaerin and Lee, Soohyun and Song, Jiwon and Cho, Aeri and Kim, Nam Wook and Seo, Jinwook},
doi = {10.1109/TVCG.2024.3413195},
file = {:C$\backslash$:/mendeley/Choe et al. - 2024 - Enhancing Data Literacy On-demand LLMs as Guides for Novices in Chart Interpretation.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools,VIS/LLM/LLM4VIS},
pages = {1--17},
publisher = {IEEE},
title = {{Enhancing Data Literacy On-demand: LLMs as Guides for Novices in Chart Interpretation}},
url = {https://ieeexplore.ieee.org/document/10555321/},
year = {2024}
}
@inproceedings{Baek2023,
archivePrefix = {arXiv},
arxivId = {2307.08985},
author = {Baek, Seungho and Im, Hyerin and Ryu, Jiseung and Park, Juhyeong and Lee, Takyeon},
booktitle = {ICML Workshop on Artificial Intelligence {\&} Human Computer Interaction},
eprint = {2307.08985},
file = {:C$\backslash$:/mendeley/Baek et al. - 2023 - PromptCrafter Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
pages = {1--5},
title = {{PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM}},
url = {https://icml.cc/virtual/2023/27410},
year = {2023}
}

@inproceedings{Liu2024d,
archivePrefix = {arXiv},
arxivId = {2411.09703},
author = {Liu, Zichen and Yu, Yue and Ouyang, Hao and Wang, Qiuyu and Cheng, Ka Leong and Wang, Wen and Liu, Zhiheng and Chen, Qifeng and Shen, Yujun},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {2411.09703},
file = {:C$\backslash$:/mendeley/Liu et al. - 2024 - MagicQuill An Intelligent Interactive Image Editing System.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
pages = {13072--13082},
title = {{MagicQuill: An Intelligent Interactive Image Editing System}},
url = {http://arxiv.org/abs/2411.09703},
year = {2025}
}
@article{Gao2024a,
author = {Gao, Weiwei and Du, Kexin and Luo, Yujia and Shi, Weinan and Yu, Chun and Shi, Yuanchun},
doi = {10.1145/3678516},
file = {:C$\backslash$:/mendeley/Gao et al. - 2024 - EasyAsk An In-App Contextual Tutorial Search Assistant for Older Adults with Voice and Touch Inputs.pdf:pdf},
journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {aug},
number = {3},
pages = {1--27},
title = {{EasyAsk: An In-App Contextual Tutorial Search Assistant for Older Adults with Voice and Touch Inputs}},
url = {https://dl.acm.org/doi/10.1145/3678516},
volume = {8},
year = {2024}
}
@inproceedings{Jung2023,
address = {New York, NY, USA},
author = {Jung, Jeesu and Seo, Hyein and Jung, Sangkeun and Chung, Riwoo and Ryu, Hwijung and Chang, Du-Seong},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
doi = {10.1145/3581641.3584057},
file = {:C$\backslash$:/mendeley/Jung et al. - 2023 - Interactive User Interface for Dialogue Summarization.pdf:pdf},
isbn = {9798400701061},
keywords = {Dialogue summarization,constraint-sensitive generation,neural networks,text tagging},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {mar},
pages = {934--957},
publisher = {ACM},
title = {{Interactive User Interface for Dialogue Summarization}},
url = {https://dl.acm.org/doi/10.1145/3581641.3584057},
year = {2023}
}
@inproceedings{Andrews2024,
address = {New York, NY, USA},
author = {Andrews, Peter and Nordberg, Oda Elise and {Zubicueta Portales}, Stephanie and Borch, Nj{\aa}l and Guribye, Frode and Fujita, Kazuyuki and Fjeld, Morten},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces, IUI'24},
doi = {10.1145/3640543.3645197},
file = {:C$\backslash$:/mendeley/Andrews et al. - 2024 - AiCommentator A Multimodal Conversational Agent for Embedded Visualization in Football Viewing.pdf:pdf},
isbn = {9798400705083},
mendeley-groups = {VIS/Animation{\&}Video/Embed Vis,VIS/Animation{\&}Video/text2video,VIS/Animation{\&}Video/Survey Tools,VIS/GenAI Interfaces/scoped instruction/tools},
month = {mar},
pages = {14--34},
publisher = {ACM},
title = {{AiCommentator: A Multimodal Conversational Agent for Embedded Visualization in Football Viewing}},
url = {https://dl.acm.org/doi/10.1145/3640543.3645197},
year = {2024}
}
@inproceedings{Kim2023d,
address = {New York, NY, USA},
author = {Kim, Tae Soo and Lee, Yoonjoo and Chang, Minsuk and Kim, Juho},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606833},
file = {:C$\backslash$:/mendeley/Kim et al. - 2023 - Cells, Generators, and Lenses Design Framework for Object-Oriented Interaction with Large Language Models.pdf:pdf},
isbn = {9798400701320},
keywords = {Generative Models,Large Language Models,Reification,Writing-Support Tool},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--18},
publisher = {ACM},
title = {{Cells, Generators, and Lenses: Design Framework for Object-Oriented Interaction with Large Language Models}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606833},
year = {2023}
}
@inproceedings{Zhang2023b,
address = {New York, NY, USA},
author = {Zhang, Zheng and Gao, Jie and Dhaliwal, Ranjodh Singh and Li, Toby Jia-Jun},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606800},
file = {:C$\backslash$:/mendeley/Zhang et al. - 2023 - VISAR A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping.pdf:pdf},
keywords = {creativity support,human-AI collaboration,writing support},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--30},
publisher = {ACM},
title = {{VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606800},
year = {2023}
}
@inproceedings{Laban2023,
address = {New York, NY, USA},
author = {Laban, Philippe and Vig, Jesse and Hearst, Marti and Xiong, Caiming and Wu, Chien-Sheng},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3654777.3676419},
file = {:C$\backslash$:/mendeley/Laban et al. - 2024 - Beyond the Chat Executable and Verifiable Text-Editing with LLMs.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--23},
publisher = {ACM},
title = {{Beyond the Chat: Executable and Verifiable Text-Editing with LLMs}},
url = {https://dl.acm.org/doi/10.1145/3654777.3676419},
year = {2024}
}
@inproceedings{Liu2024b,
address = {New York, NY, USA},
author = {Liu, Yiren and Chen, Si and Cheng, Haocong and Yu, Mengxia and Ran, Xiao and Mo, Andrew and Tang, Yiliu and Huang, Yun},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642698},
file = {:C$\backslash$:/mendeley/Liu et al. - 2024 - How AI Processing Delays Foster Creativity Exploring Research Question Co-Creation with an LLM-based Agent.pdf:pdf},
mendeley-groups = {VIS/EDU/SRL,VIS/GenAI Interfaces/scoped instruction/tools,VIS/EDU,VIS/EDU/AI4Research},
month = {may},
pages = {1--25},
publisher = {ACM},
title = {{How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642698},
year = {2024}
}
@inproceedings{Angert2023,
address = {New York, NY, USA},
author = {Angert, Tyler and Suzara, Miroslav and Han, Jenny and Pondoc, Christopher and Subramonyam, Hariharan},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606719},
file = {:C$\backslash$:/mendeley/Angert et al. - 2023 - Spellburst A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces,VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--22},
publisher = {ACM},
title = {{Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606719},
year = {2023}
}
@inproceedings{Wu2023c,
address = {New York, NY, USA},
author = {Wu, Sherry and Shen, Hua and Weld, Daniel S. and Heer, Jeffrey and Ribeiro, Marco Tulio},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
doi = {10.1145/3581641.3584059},
file = {:C$\backslash$:/mendeley/Wu et al. - 2023 - ScatterShot Interactive In-context Example Curation for Text Transformation.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {mar},
pages = {353--367},
publisher = {ACM},
title = {{ScatterShot: Interactive In-context Example Curation for Text Transformation}},
url = {https://dl.acm.org/doi/10.1145/3581641.3584059},
year = {2023}
}
@inproceedings{Wang2024g,
address = {New York, NY, USA},
author = {Wang, Zhijie and Huang, Yuheng and Song, Da and Ma, Lei and Zhang, Tianyi},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642803},
file = {:C$\backslash$:/mendeley/Wang et al. - 2024 - PromptCharm Text-to-Image Generation through Multi-modal Prompting and Refinement.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--21},
publisher = {ACM},
title = {{PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642803},
year = {2024}
}
@inproceedings{Masson2023b,
address = {New York, NY, USA},
author = {Masson, Damien and Malacria, Sylvain and Casiez, G{\'{e}}ry and Vogel, Daniel},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613904.3642462},
file = {:C$\backslash$:/mendeley/Masson et al. - 2024 - DirectGPT A Direct Manipulation Interface to Interact with Large Language Models.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--16},
publisher = {ACM},
title = {{DirectGPT: A Direct Manipulation Interface to Interact with Large Language Models}},
url = {https://dl.acm.org/doi/10.1145/3613904.3642462},
year = {2024}
}
@inproceedings{Zhu,
address = {New York, NY, USA},
author = {Yen, Ryan and Zhu, Jiawen Stefanie and Suh, Sangho and Xia, Haijun and Zhao, Jian},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3654777.3676357},
file = {:C$\backslash$:/mendeley/Yen et al. - 2024 - CoLadder Manipulating Code Generation via Multi-Level Blocks.pdf:pdf},
isbn = {9798400706288},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--20},
publisher = {ACM},
title = {{CoLadder: Manipulating Code Generation via Multi-Level Blocks}},
url = {https://dl.acm.org/doi/10.1145/3654777.3676357},
year = {2024}
}
@article{dataplaywright,
author = {Shen, Leixian and Li, Haotian and Wang, Yun and Luo, Tianqi and Luo, Yuyu and Qu, Huamin},
doi = {10.1109/TVCG.2024.3477926},
file = {:C$\backslash$:/mendeley/Shen et al. - 2024 - Data Playwright Authoring Data Videos With Annotated Narration.pdf:pdf},
journal = {IEEE Transactions on Visualization and Computer Graphics},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/Animation{\&}Video/Survey Tools,VIS/AAA Mine,VIS/GenAI Interfaces/scoped instruction/tools},
pages = {1--14},
title = {{Data Playwright: Authoring Data Videos With Annotated Narration}},
url = {https://ieeexplore.ieee.org/document/10720675/},
year = {2024}
}
@inproceedings{Chen2023e,
address = {New York, NY, USA},
author = {Chen, Weihao and Yu, Chun and Wang, Huadong and Wang, Zheng and Yang, Lichen and Wang, Yukun and Shi, Weinan and Shi, Yuanchun},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3586183.3606741},
file = {:C$\backslash$:/mendeley/Chen et al. - 2023 - From Gap to Synergy Enhancing Contextual Understanding through Human-Machine Collaboration in Personalized Systems.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--15},
publisher = {ACM},
title = {{From Gap to Synergy: Enhancing Contextual Understanding through Human-Machine Collaboration in Personalized Systems}},
url = {https://dl.acm.org/doi/10.1145/3586183.3606741},
year = {2023}
}
@inproceedings{Ma2024,
address = {New York, NY, USA},
author = {Ma, Xiao and Mishra, Swaroop and Liu, Ariel and Su, Sophie Ying and Chen, Jilin and Kulkarni, Chinmay and Cheng, Heng-Tze and Le, Quoc and Chi, Ed},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3613905.3651093},
file = {:C$\backslash$:/mendeley/Ma et al. - 2024 - Beyond ChatBots ExploreLLM for Structured Thoughts and Personalized Model Responses.pdf:pdf},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
pages = {1--12},
publisher = {ACM},
title = {{Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses}},
url = {https://dl.acm.org/doi/10.1145/3613905.3651093},
year = {2024}
}
@inproceedings{Liu2023e,
address = {New York, NY, USA},
author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3544548.3580817},
file = {:C$\backslash$:/mendeley/Liu et al. - 2023 - “What It Wants Me To Say” Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large L(2).pdf:pdf},
isbn = {9781450394215},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/tools},
month = {apr},
pages = {1--31},
publisher = {ACM},
title = {{“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models}},
url = {https://dl.acm.org/doi/10.1145/3544548.3580817},
year = {2023}
}
@inproceedings{NotePlayer,
author = {Ouyang, Yang and Shen, Leixian and Wang, Yun and Li, Quan},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3654777.3676410},
file = {:C$\backslash$:/mendeley/Ouyang et al. - 2024 - NotePlayer Engaging Computational Notebooks for Dynamic Presentation of Analytical Processes.pdf:pdf},
mendeley-groups = {VIS/AAA Mine,VIS/GenAI Interfaces/scoped instruction/tools},
month = {oct},
pages = {1--20},
publisher = {ACM},
title = {{NotePlayer: Engaging Computational Notebooks for Dynamic Presentation of Analytical Processes}},
url = {http://dx.doi.org/10.1145/3654777.3676410},
year = {2024}
}
@article{Wang2024i,
author = {Wang, Zeyu and Shi, Yuanchun and Wang, Yuntao and Yao, Yuchen and Yan, Kun and Wang, Yuhan and Ji, Lei and Xu, Xuhai and Yu, Chun},
doi = {10.1145/3659623},
file = {:C$\backslash$:/mendeley/Wang et al. - 2024 - G-VOILA Gaze-Facilitated Information Querying in Daily Scenarios.pdf:pdf},
journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction,VIS/GenAI Interfaces/scoped instruction/tools},
month = {may},
number = {2},
pages = {1--33},
publisher = {ACM},
title = {{G-VOILA: Gaze-Facilitated Information Querying in Daily Scenarios}},
url = {https://dl.acm.org/doi/10.1145/3659623},
volume = {8},
year = {2024}
}


@inproceedings{Park2025,
abstract = {Generative AI (GenAI) tools are increasingly integrated into design workflows. While text prompts remain the primary input method for GenAI image tools, designers often struggle to craft effective ones. Moreover, research has primarily focused on input methods for ideation, with limited attention to refinement tasks. This study explores designers' preferences for three input methods—text prompts, annotations, and scribbles—through a preliminary digital paper-based study with seven professional designers. Designers preferred annotations for spatial adjustments and referencing in-image elements, while scribbles were favored for specifying attributes such as shape, size, and position, often combined with other methods. Text prompts excelled at providing detailed descriptions or when designers sought greater GenAI creativity. However, designers expressed concerns about AI misinterpreting annotations and scribbles and the effort needed to create effective text prompts. These insights inform GenAI interface design to better support refinement tasks, align with workflows, and enhance communication with AI systems.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2503.03398},
author = {Park, Hyerim and Eiband, Malin and Luckow, Andre and Sedlmair, Michael},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706599.3719802},
eprint = {2503.03398},
file = {:C$\backslash$:/mendeley/Park et al. - Unknown - Exploring Visual Prompts Refining Images with Scribbles and Annotations in Generative AI Image Tools.pdf:pdf},
isbn = {9798400713958},
keywords = {annotation-based input,design refinement,generative AI,scribble-based input,text prompts},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {apr},
number = {1},
pages = {1--10},
publisher = {ACM},
title = {{Exploring Visual Prompts: Refining Images with Scribbles and Annotations in Generative AI Image Tools}},
url = {https://dl.acm.org/doi/10.1145/3706599.3719802},
volume = {1},
year = {2025}
}
@inproceedings{Gmeiner2025,
abstract = {Despite Generative AI (GenAI) systems' potential for enhancing content creation, users often struggle to effectively integrate GenAI into their creative workflows. Core challenges include misalignment of AI-generated content with user intentions (intent elicitation and alignment), user uncertainty around how to best communicate their intents to the AI system (prompt formulation), and insufficient flexibility of AI systems to support diverse creative workflows (workflow flexibility). Motivated by these challenges, we created IntentTagger: a system for slide creation based on the notion of Intent Tags - small, atomic conceptual units that encapsulate user intent - for exploring granular and non-linear micro-prompting interactions for Human-GenAI co-creation workflows. Our user study with 12 participants provides insights into the value of flexibly expressing intent across varying levels of ambiguity, meta-intent elicitation, and the benefits and challenges of intent tag-driven workflows. We conclude by discussing the broader implications of our findings and design considerations for GenAI-supported content creation workflows.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2502.18737},
author = {Gmeiner, Frederic and Marquardt, Nicolai and Bentley, Michael and Romat, Hugo and Pahud, Michel and Brown, David and Roseway, Asta and Martelaro, Nikolas and Holstein, Kenneth and Hinckley, Ken and Riche, Nathalie},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706598.3713861},
eprint = {2502.18737},
file = {:C$\backslash$:/mendeley/Gmeiner et al. - 2025 - Intent Tagging Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows.pdf:pdf},
isbn = {9798400713941},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw,VIS/GenAI Interfaces/layer/paradigm},
month = {apr},
pages = {1--31},
publisher = {ACM},
title = {{Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows}},
url = {https://dl.acm.org/doi/10.1145/3706598.3713861},
year = {2025}
}
@inproceedings{Peng2024,
abstract = {Visually oriented designers often struggle to create efective generative AI (GenAI) prompts. A preliminary study identifed specifc issues in composing and fne-tuning prompts, as well as needs in accurately translating intentions into rich input. We developed DesignPrompt, a moodboard tool that lets designers combine multiple modalities — images, color, text — into a single GenAI prompt and tweak the results. We ran a comparative structured observation study with 12 professional designers to better understand their intent expression, expectation alignment and transparency perception using DesignPrompt and text input GenAI. We found that multimodal prompt input encouraged designers to explore and express themselves more efectively. Designer's interaction preferences change according to their overall sense of control over the GenAI and whether they are seeking inspiration or a specifc image. Designers developed innovative uses of DesignPrompt, including developing elaborate multimodal prompts and creating a multimodal prompt pattern to maximize novelty while ensuring consistency.},
address = {New York, NY, USA},
author = {Peng, Xiaohan and Koch, Janin and Mackay, Wendy E.},
booktitle = {Designing Interactive Systems Conference},
doi = {10.1145/3643834.3661588},
file = {:C$\backslash$:/mendeley/Peng, Koch, Mackay - 2024 - DesignPrompt Using Multimodal Interaction for Design Exploration with Generative AI.pdf:pdf},
isbn = {9798400705830},
keywords = {Creativity Support Tool,Design Practice,Generative AI,Human-AI Ideation,Human-AI Interaction,Moodboard},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {jul},
number = {June},
pages = {804--818},
publisher = {ACM},
title = {{DesignPrompt: Using Multimodal Interaction for Design Exploration with Generative AI}},
url = {https://dl.acm.org/doi/10.1145/3643834.3661588},
year = {2024}
}
@inproceedings{Lin2025a,
abstract = {Text-to-image models can generate visually appealing images from text descriptions. Efforts have been devoted to improving model controls with prompt tuning and spatial conditioning. However, our formative study highlights the challenges for non-expert users in crafting appropriate prompts and specifying fine-grained spatial conditions (e.g., depth or canny references) to generate semantically cohesive images, especially when multiple objects are involved. In response, we introduce SketchFlex, an interactive system designed to improve the flexibility of spatially conditioned image generation using rough region sketches. The system automatically infers user prompts with rational descriptions within a semantic space enriched by crowd-sourced object attributes and relationships. Additionally, SketchFlex refines users' rough sketches into canny-based shape anchors, ensuring the generation quality and alignment of user intentions. Experimental results demonstrate that SketchFlex achieves more cohesive image generations than end-to-end models, meanwhile significantly reducing cognitive load and better matching user intentions compared to region-based generation baseline.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2502.07556},
author = {Lin, Haichuan and Ye, Yilin and Xia, Jiazhi and Zeng, Wei},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706598.3713801},
eprint = {2502.07556},
file = {:C$\backslash$:/mendeley/Lin et al. - 2025 - SketchFlex Facilitating Spatial-Semantic Coherence in Text-to-Image Generation with Region-Based Sketches.pdf:pdf},
isbn = {9798400713941},
keywords = {2025,Diffusion mode,Generative artificial intelligence,acm reference format,and wei zeng,diffusion model,fa-,generative artificial intelligence,haichuan lin,jiazhi xia,sketchflex,yilin ye},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {apr},
number = {1},
pages = {1--19},
publisher = {ACM},
title = {{SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image Generation with Region-Based Sketches}},
url = {https://dl.acm.org/doi/10.1145/3706598.3713801},
volume = {1},
year = {2025}
}
@inproceedings{Shi2025,
abstract = {Expressing design intent using natural language prompts requires designers to verbalize the ambiguous visual details concisely, which can be challenging or even impossible. To address this, we introduce Brickify, a visual-centric interaction paradigm - expressing design intent through direct manipulation on design tokens. Brickify extracts visual elements (e.g., subject, style, and color) from reference images and converts them into interactive and reusable design tokens that can be directly manipulated (e.g., resize, group, link, etc.) to form the visual lexicon. The lexicon reflects users' intent for both what visual elements are desired and how to construct them into a whole. We developed Brickify to demonstrate how AI models can interpret and execute the visual lexicon through an end-to-end pipeline. In a user study, experienced designers found Brickify more efficient and intuitive than text-based prompts, allowing them to describe visual details, explore alternatives, and refine complex designs with greater ease and control.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2502.21219},
author = {Shi, Xinyu and Wang, Yinghou and Rossi, Ryan and Zhao, Jian},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706598.3714087},
eprint = {2502.21219},
file = {:C$\backslash$:/mendeley/Shi et al. - 2025 - Brickify Enabling Expressive Design Intent Specification through Direct Manipulation on Design Tokens.pdf:pdf},
isbn = {9798400713941},
keywords = {Design Intent Expression,Direct Manipulation,Interaction Techniques,Interactive Design Token},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {apr},
pages = {1--20},
publisher = {ACM},
title = {{Brickify: Enabling Expressive Design Intent Specification through Direct Manipulation on Design Tokens}},
url = {https://dl.acm.org/doi/10.1145/3706598.3714087},
year = {2025}
}
@inproceedings{Zhou2023,
abstract = {Visual programming has the potential of providing novice programmers with a low-code experience to build customized processing pipelines. Existing systems typically require users to build pipelines from scratch, implying that novice users are expected to set up and link appropriate nodes from a blank workspace. In this paper, we introduce InstructPipe, an AI assistant for prototyping machine learning (ML) pipelines with text instructions. We contribute two large language model (LLM) modules and a code interpreter as part of our framework. The LLM modules generate pseudocode for a target pipeline, and the interpreter renders the pipeline in the node-graph editor for further human-AI collaboration. Both technical and user evaluation (N=16) shows that InstructPipe empowers users to streamline their ML pipeline workflow, reduce their learning curve, and leverage open-ended commands to spark innovative ideas.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2312.09672},
author = {Zhou, Zhongyi and Jin, Jing and Phadnis, Vrushank and Yuan, Xiuxiu and Jiang, Jun and Qian, Xun and Wright, Kristen and Sherwood, Mark and Mayes, Jason and Zhou, Jingtao and Huang, Yiyi and Xu, Zheng and Zhang, Yinda and Lee, Johnny and Olwal, Alex and Kim, David and Iyengar, Ram and Li, Na and Du, Ruofei},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706598.3713905},
eprint = {2312.09672},
file = {:C$\backslash$:/mendeley/Zhou et al. - 2023 - InstructPipe Building Visual Programming Pipelines with Human Instructions.pdf:pdf},
isbn = {9798400713941},
keywords = {Deep Learning,Deep Neural Networks,Graph Compiler,Large Language Models,Low-code Development,Node-graph Editor,Visual Analytics,Visual Programming,Visual Prototyping},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {apr},
pages = {1--22},
publisher = {ACM},
title = {{InstructPipe: Generating Visual Blocks Pipelines with Human Instructions and LLMs}},
url = {https://dl.acm.org/doi/10.1145/3706598.3713905},
year = {2025}
}
@inproceedings{Chen2025,
abstract = {Motion comics, a digital animation format that enhances comic book narratives, has wide applications in storytelling, education, and advertising. However, their creation poses significant challenging for amateur creators, primarily due to the need for specialized skills and complex workflows. To address these issues, we conducted an exploratory survey (N = 58) to understand challenges associated with creating motion comics, and an expert interview (N = 4) to identify a typical workflow for creation. We further analyzed 95 online motion comics to gain insights into the design space of character and object actions. Based on our findings, we proposed DancingBoard, an integrated authoring tool designed to simplify the creation process. This tool features a user-friendly interface and a guided workflow, providing comprehensive support throughout each step of the creation process. A user study involving 23 creators showed that, comparing to professional tools, DancingBoard is easily comprehensible and provides improved guidance and support, requiring less efforts from users. Additionally, a separate study with 18 audience members confirmed the tool's effectiveness in conveying the story to its viewers.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:2503.09061v1},
author = {Chen, Longfei and Li, Shengxin and Li, Ziang and Li, Quan},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
doi = {10.1145/3708359.3712167},
eprint = {arXiv:2503.09061v1},
file = {:C$\backslash$:/mendeley/Chen, Li - 2025 - DancingBoard Streamlining the Creation of Motion Comics to Enhance Narratives.pdf:pdf},
isbn = {9798400713064},
keywords = {Motion comic,authoring tool,creative design,visual storytelling},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {mar},
pages = {477--503},
publisher = {ACM},
title = {{DancingBoard: Streamlining the Creation of Motion Comics to Enhance Narratives}},
url = {https://dl.acm.org/doi/10.1145/3708359.3712167},
year = {2025}
}
@inproceedings{Chen2025a,
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2503.16434},
author = {Lee, Jimin and Chen, Steven-Shine and Liang, Paul Pu},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706599.3719790},
eprint = {2503.16434},
file = {:C$\backslash$:/mendeley/Chen, Lee, Liang - 2025 - Interactive Sketchpad An Interactive Multimodal System for Collaborative, Visual Problem-Solving.pdf:pdf},
isbn = {9798400713958},
keywords = {AI for education,multimodal interaction,vision-language models},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {apr},
number = {1},
pages = {1--14},
publisher = {ACM},
title = {{Interactive Sketchpad: A Multimodal Tutoring System for Collaborative, Visual Problem-Solving}},
url = {https://dl.acm.org/doi/10.1145/3706599.3719790},
volume = {1},
year = {2025}
}
@inproceedings{Epperson2025,
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:2503.02068v1},
author = {Epperson, Will and Bansal, Gagan and Dibia, Victor C and Fourney, Adam and Gerrits, Jack and Zhu, Erkang (Eric) and Amershi, Saleema},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706598.3713581},
eprint = {arXiv:2503.02068v1},
file = {:C$\backslash$:/mendeley/Epperson et al. - 2025 - Interactive Debugging and Steering of Multi-Agent AI Systems.pdf:pdf},
isbn = {9798400713941},
keywords = {AI agents,ai agents,ai debugging,interactive debugging sys,interactive debugging systems,language},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {apr},
number = {1},
pages = {1--15},
publisher = {ACM},
title = {{Interactive Debugging and Steering of Multi-Agent AI Systems}},
url = {https://dl.acm.org/doi/10.1145/3706598.3713581},
volume = {1},
year = {2025}
}
@inproceedings{Riche2025,
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2502.18736},
author = {Riche, Nathalie and Offenwanger, Anna and Gmeiner, Frederic and Brown, David and Romat, Hugo and Pahud, Michel and Marquardt, Nicolai and Inkpen, Kori and Hinckley, Ken},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3706598.3714259},
eprint = {2502.18736},
file = {:C$\backslash$:/mendeley/Riche et al. - 2025 - AI-Instruments Embodying Prompts as Instruments to Abstract {\&} Reflect Graphical Interface Commands as General-Purp.pdf:pdf},
isbn = {9798400713941},
keywords = {generative AI interfaces,instrumental interaction},
mendeley-groups = {VIS/GenAI Interfaces/HAI,VIS/GenAI Interfaces/layer/theory,VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {apr},
pages = {1--18},
publisher = {ACM},
title = {{AI-Instruments: Embodying Prompts as Instruments to Abstract & Reflect Graphical Interface Commands as General-Purpose Tools}},
url = {https://dl.acm.org/doi/10.1145/3706598.3714259},
year = {2025}
}
@article{You2025,
abstract = {Creative design is an inherently complex and iterative process characterized by continuous exploration, evaluation, and refinement. While recent advances in generative AI have demonstrated remarkable potential in supporting specific design tasks, there remains a critical gap in understanding how these technologies can enhance the holistic design process rather than just isolated stages. This paper introduces DesignManager, a novel AI-powered design support system that aims to transform how designers collaborate with AI throughout their creative workflow. Through a formative study examining designers' current practices with generative AI, we identified key challenges and opportunities in integrating AI into the creative design process. Based on these insights, we developed DesignManager as an interactive copilot system that provides node-based visualization of design evolution, enabling designers to track, modify, and branch their design processes while maintaining meaningful dialogue-based collaboration. The system offers two collaboration modes: DesignManager-guiding and Designer-guiding. Designers can engage in conversational interactions with the DesignManager to obtain design inspiration and tool recommendations, and proactively advance the design progress. The system employs an agent framework to manage decoupled contextual information emerged during the design process, facilitating deep understanding of designers' needs and providing context-aware assistance. Our technical evaluation validated the effectiveness of context decoupling and the use of agent framework, while the open-ended user study with experts demonstrated that DesignManager successfully supports intuitive intention expression, flexible process control, and deeper creative articulation. This work contributes to the understanding of how AI can evolve from task-specific tools to collaborative partners in creative design processes.},
author = {You, Weitao and Lu, Yinyu and Ma, Zirui and Li, Nan and Zhou, Mingxu and Zhao, Xue and Chen, Pei and Sun, Lingyun},
doi = {10.1145/3730919},
file = {:C$\backslash$:/mendeley/3730919.pdf:pdf},
issn = {0730-0301},
journal = {ACM Transactions on Graphics},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {aug},
number = {4},
pages = {1--26},
title = {{DesignManager: An Agent-Powered Copilot for Designers to Integrate AI Design Tools into Creative Workflows}},
url = {https://dl.acm.org/doi/10.1145/3730919},
volume = {44},
year = {2025}
}
@inproceedings{Yen2024,
abstract = {We present an initial step towards building a system for programmers to edit code using free-form sketch annotations drawn directly onto editor and output windows. Using a working prototype system as a technical probe, an exploratory study (N = 6) examines how programmers sketch to annotate Python code to communicate edits for an AI model to perform. The results reveal personalized work-flow strategies and how similar annotations vary in abstractness and intention across different scenarios and users.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2502.03719},
author = {Yen, Ryan and Zhao, Jian and Vogel, Daniel},
booktitle = {The 37th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3672539.3686324},
eprint = {2502.03719},
file = {:C$\backslash$:/mendeley/Yen, Zhao, Vogel - 2024 - Code Shaping Iterative Code Editing with Free-form Sketching.pdf:pdf},
isbn = {9798400707186},
keywords = {ink-based sketching,programming interface},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {oct},
pages = {1--3},
publisher = {ACM},
title = {{Code Shaping: Iterative Code Editing with Free-form Sketching}},
url = {https://dl.acm.org/doi/10.1145/3672539.3686324},
year = {2024}
}
@article{Nguyen2023,
abstract = {An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.},
archivePrefix = {arXiv},
arxivId = {2503.02003},
author = {Nguyen, Tin and Bolton, Logan and Taesiri, Mohammad Reza and Nguyen, Anh Totti},
eprint = {2503.02003},
file = {:C$\backslash$:/mendeley/Nguyen et al. - 2025 - HoT Highlighted Chain of Thought for Referencing Supportive Facts from Inputs.pdf:pdf},
journal = {arXiv},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
number = {2},
title = {{HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs}},
url = {http://arxiv.org/abs/2503.02003},
year = {2025}
}
@article{Kaputa2025,
abstract = {Programming-by-prompting with generative AI offers a new paradigm for end-user programming, shifting the focus from syntactic fluency to semantic intent. This shift holds particular promise for non-programmers such as educators, who can describe instructional goals in natural language to generate interactive learning content. Yet in bypassing direct code authoring, many of programming's core affordances - such as traceability, stepwise refinement, and behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA) framework as a way to recover these affordances while preserving the expressive flexibility of natural language. CoA decomposes the synthesis process into a sequence of cognitively meaningful, task-aligned representations that function as checkpoints for specification, inspection, and refinement. We instantiate this approach in SimStep, an authoring environment for teachers that scaffolds simulation creation through four intermediate abstractions: Concept Graph, Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address ambiguities and misalignments, SimStep includes an inverse correction process that surfaces in-filled model assumptions and enables targeted revision without requiring users to manipulate code. Evaluations with educators show that CoA enables greater authoring control and interpretability in programming-by-prompting workflows.},
archivePrefix = {arXiv},
arxivId = {2507.09664},
author = {Kaputa, Zoe and Rajaram, Anika and Feliciano, Vryan Almanon and Lyu, Zhuoyue and Agrawala, Maneesh and Subramonyam, Hari},
eprint = {2507.09664},
file = {:C$\backslash$:/mendeley/2507.09664v1.pdf:pdf},
journal = {arXiv},
mendeley-groups = {VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
publisher = {arXiv},
title = {{SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations}},
url = {http://arxiv.org/abs/2507.09664},
year = {2025}
}
@inproceedings{Jacobs2025,
address = {New York, NY, USA},
author = {Bourgault, Samuelle and Wei, Li-Yi and Jacobs, Jennifer and Kazi, Rubaiat Habib},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
doi = {10.1145/3715336.3735766},
file = {:C$\backslash$:/mendeley/3715336.3735766.pdf:pdf},
isbn = {9798400714856},
mendeley-groups = {VIS/Animation{\&}Video/Animation,VIS/GenAI Interfaces/scoped instruction/new tool after lbw},
month = {jul},
pages = {1366--1386},
publisher = {ACM},
title = {{Narrative Motion Blocks: Combining Direct Manipulation and Natural Language Interactions for Animation Creation}},
url = {https://dl.acm.org/doi/10.1145/3715336.3735766},
year = {2025}
}
@inproceedings{Zhang2025,
author = {Zhang, Runhua and Ouyang, Yang and Shen, Leixian and Tang, Yuying and Ma, Xiaojuan and Qu, Huamin and Xu, Xian},
booktitle = {The 38th Annual ACM Symposium on User Interface Software and Technology, UIST'25},
doi = {10.1145/3746059.3747713},
file = {:C$\backslash$:/mendeley/paperbridge.pdf:pdf},
isbn = {9798400720376},
mendeley-groups = {VIS/AAA Mine},
pages = {1--21},
title = {{PaperBridge : Crafting Research Narratives through}},
year = {2025}
}
@inproceedings{Zhang2025a,
author = {Zhang, Wenshuo and Shen, Leixian and Xu, Shuchang and Wang, Jindu and Zhao, Jian and Qu, Huamin and Yuan, Lin-Ping},
booktitle = {The 38th Annual ACM Symposium on User Interface Software and Technology, UIST'25},
doi = {10.1145/3746059.3747668},
file = {:C$\backslash$:/mendeley/NeuroSync.pdf:pdf},
isbn = {9798400720376},
keywords = {Bidirectional Ambiguity,Coding,Distillation,Graph Representation,Human-LLM Alignment,acm reference format,bidirectional ambiguity,coding,distillation,graph,human-llm alignment,representation},
mendeley-groups = {VIS/AAA Mine},
pages = {1--19},
publisher = {Association for Computing Machinery},
title = {{NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification}},
year = {2025}
}
@article{Gao2025,
author = {Gao, Lin and Shen, Leixian and Zhao, Yuheng and Lan, Jiexiang and Qu, Huamin and Chen, Siming},
file = {:C$\backslash$:/mendeley/SceneLoom.pdf:pdf},
journal = {IEEE Transactions on Visualization and Computer Graphics},
mendeley-groups = {VIS/AAA Mine},
pages = {1--11},
title = {{SceneLoom : Communicating Data with Scene Context}},
year = {2025}
}



